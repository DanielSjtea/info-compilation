{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129b0344-b56c-4c27-8a3d-a40cbf8f8724",
   "metadata": {},
   "source": [
    "# Linear Algebra Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b232093-9a11-4520-9fa3-5e2913c441ff",
   "metadata": {},
   "source": [
    "- Dot Product: $\\langle u, Av \\rangle = u^TAv = \\sum_{i=1}^{d}u_i(Av)_i = \\sum_{i=1}^{d}\\sum_{j=1}^{d}u_iA_{ij}v_j$\n",
    "- Norm: $\\|u\\| = \\langle u, u \\rangle = u_1^2 + \\dots + u_d^2$\n",
    "- Determinants: $det(AB) = det(A) \\cdot det(B)$, $det(A^{-1}) = (det(A))^{-1}$\n",
    "- Eigenvalues: exists a nonzero vector $v$ such that $Av = \\lambda v$\n",
    "    - Only exists for square matrices\n",
    "- Orthogonal Matrices:\n",
    "    - $\\|Ov\\| = \\|v\\|$\n",
    "    - $O^{-1} = O^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fae1f-5b5a-4086-a098-5cbe9d5d7de5",
   "metadata": {},
   "source": [
    "## Positive Semi-definite Matrices (psd)\n",
    "- Matrix is positive semi-definite (psd) if $\\langle x, Mx\\rangle \\geq 0$\n",
    "- Matrix is positive definite (pd) if $\\langle x, Mx\\rangle > 0$\n",
    "\n",
    "## Symmetric Matrices\n",
    "- Can be **diagonalised**: $S = ODO^T$ where $O$ is an orthogonal matrix and $D = Diag(\\lambda_1,\\dots,\\lambda_d)$ is a diagonal matrix\n",
    "- Is **psd** iff ALL $\\lambda \\geq 0$\n",
    "- Is **pd** iff ALL $\\lambda > 0$\n",
    "\n",
    "## Invertible Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b0b82-cdcb-4f90-8572-49a9122fae6d",
   "metadata": {},
   "source": [
    "# Calculus Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a514566-212f-449c-bfea-b656d109a754",
   "metadata": {},
   "source": [
    "### First Order Derivative:\n",
    "$$\n",
    "f(x+\\epsilon) \\approx f(x) + \\langle \\nabla f(x), \\epsilon \\rangle\n",
    "$$\n",
    "\n",
    "### Second Order Derivative\n",
    "$$\n",
    "f(x+\\epsilon) \\approx f(x) + \\langle \\nabla f(x), \\epsilon \\rangle + \\frac{1}{2}\\langle\\epsilon, Hess(x) \\epsilon\\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbf36e-4811-48eb-9e66-7c600a848aad",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b6e1e-e7ad-45ec-b018-a6641731e344",
   "metadata": {},
   "source": [
    "## Convexity\n",
    "Function is **convex** if:\n",
    "$$\n",
    "f(\\lambda x + (1 - \\lambda) y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\n",
    "$$\n",
    "Second Derivative Criterion:\n",
    "- If $\\text{Hess}_F(x)$ is **psd** then function is **convex**\n",
    "- If $\\text{Hess}_F(x)$ is **pd** then function is **strictly convex**\n",
    "    - For a strictly convex function, local minimum = global minimum\n",
    "    \n",
    "### Convexity Preserving Operations:\n",
    "- Nonnegative Weighted Sums\n",
    "    - If $f_1,\\dots,f_n$ are convex functions and $w_1, \\dots, w_n \\geq 0$ then,  \n",
    "    $f(x) = w_1f_1(x) + \\dots + w_nf_n(x)$ is also convex\n",
    "- Composition with an affine mapping\n",
    "    - $g(x) = f(Ax + b)$, if $f$ is convex, then $g$ is also convex\n",
    "- Pointwise Maximum\n",
    "- Restriction to a line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc2741-95e8-4823-b7d1-9a401a9221e1",
   "metadata": {},
   "source": [
    "## Advantages of Probabilistic Output\n",
    "- Making an error of one type may be more **\"expensive\"** than another (e.g cancer vs no cancer)\n",
    "- Can tell if we are **uncertain** about our prediction and refuse to classify it if so, and pass the case to a human expert\n",
    "- Easier to **combine several probabilistic outputs** given by several models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d09249-572d-4b11-9d9e-62216c338e6f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4496b7ad-7e3c-4229-a873-c6db566d2ab1",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Logisitic Regression\n",
    "$$\n",
    "P(y=y_0|x) = \\frac{1}{1+exp(-y_0\\langle\\beta,x\\rangle)}\\\\\n",
    "\\text{Log-likelihood} = - \\sum^N_{i=1}log(1 + exp(-y_0\\langle\\beta,x\\rangle))\\\\\n",
    "\\text{MLE }\\beta_* = argmin\\left\\{\\sum^N_{i=1}log(1 + exp(-y_0\\langle\\beta,x\\rangle))\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e8c72-1b8e-4daf-b904-cec8149cad4c",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "### Rate of Convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
