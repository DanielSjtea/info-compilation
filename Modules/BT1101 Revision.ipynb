{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processes for Problem Solving\n",
    "1. Recognizing a problem\n",
    "2. Defining the problem\n",
    "3. Structuring the problem\n",
    "4. Analyzing the problem\n",
    "5. Interpreting results & making a decision\n",
    "6. Implementing the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep (use %>% to pipe)\n",
    "count(columns)\n",
    "spread(key=`col`, value=`col`) #change from long to wide form\n",
    "pivot_wider()\n",
    "pivot_longer()\n",
    "summarize() #can use with min, max, sum, var, sd, first, n, n_distinct\n",
    "\n",
    "#Types\n",
    "as.factor(data$col, levels=c(\"val1\", \"val2\"), ordered=T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analytics\n",
    "**What Happened?**  \n",
    "Using data to understand current and past business performance and make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations in R\n",
    "- **Categorical** data:\n",
    "    - Bar Chart\n",
    "    - Pie Chart\n",
    "- **Continuous** data:\n",
    "    - Histogram\n",
    "- View **Relationships between Variables**:\n",
    "    - Scatterplot\n",
    "- Visualise **trends**:\n",
    "    - Line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ggplot2\n",
    "geom_bar(stat = 'identity', position='dodge/stack(default)') # Bar Charts\n",
    "geom_histogram(fill='color', bins=30) # Histogram\n",
    "geom_line() # Line Charts\n",
    "geom_point() # Scatterplots\n",
    "\n",
    "## Labels\n",
    "labs(title ='', x ='', y ='', fill = '')\n",
    "## For displaying tables\n",
    "kable(data, caption='')\n",
    "\n",
    "## For Multiple plots\n",
    "library(gridExtra)\n",
    "require(gridExtra)\n",
    "grid.arrange(plot1, plot2, ncol=2, nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "- Frequency Table: displays number of observations in **each group**\n",
    "- Cumulative Relative Frequency: **proportion of total** observations that fall at or below the upper limit of each group\n",
    "- Contigency Table: displays number of observations in **each category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pareto Analysis\n",
    "pareto_data <- PCB_data %>% \n",
    "  select(Amount) %>% \n",
    "  arrange(desc(Amount)) %>% \n",
    "  mutate(Percentage = Amount/sum(Amount),  Cumulative = cumsum(Percentage)) \n",
    "### find first row that has 80% cumulative\n",
    "pareto_row <- min(which(pareto_data$Cumulative > 0.8))\n",
    "pareto_row / nrow(pareto_data)\n",
    "\n",
    "## Contingency Tables\n",
    "data.spread <- data %>% count(col, col) %>% spread(key=col, value=n)\n",
    "kable(data.spread, caption=\"Contigency Table for ...\")\n",
    "\n",
    "## Interactive Contingency Table\n",
    "library(rpivotTable)\n",
    "rpivotTable(data, rows=c('header'), cols=c('header'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distributions & Data Modelling\n",
    "- Classical definition: probabilities can be deduced from theoretical arguments\n",
    "- Relative frequency definition: probabilities are based on empirical data\n",
    "- Subjective definition: probabilities are based on judgment and experience\n",
    "\n",
    "### Descriptive Statistics\n",
    "- Kurtosis: +ve $\\rightarrow$ distribution is peaked with less dispersion\n",
    "- Coefficient of Skewness: +ve and >1 $\\rightarrow$ high-degree of right skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnorm(1000, 750, 100) ## to get P(X < 1000) with X ~ N(750, 100^2)\n",
    "pnorm(1000, 750, 100, lower.tail = FALSE) == 1 - pnorm(1000, 750, 100) ## to get P(X > 1000)\n",
    "pnorm(1000, 750, 100) - pnorm(800, 750, 100) ## to get P(800 < X < 1000)\n",
    "\n",
    "# Generate Descriptive Statistics\n",
    "require(psych)\n",
    "describe(col, data)\n",
    "describeBy(data$col1, group=data$col2, mat=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-hoc Tests: Goodness of Fit\n",
    "- Kolmogorow-Smirnov (works well for small samples and only for non-parametric data)\n",
    "- Chi-square (needs at least 50 data points)\n",
    "- Anderson-Darling (puts more weight on the differences between the tails of the distributions)\n",
    "- Shapiro-Wilk Test (test data against normal distribution, check normality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorow-Smirnov test\n",
    "# Chi-square test\n",
    "# Anderson Darling test\n",
    "# Shapiro-Wilk test\n",
    "shapiro.test(data) # if W close to 1, p-value > 0.05 => implies data does not deviate from normality\n",
    "\n",
    "# Density plot\n",
    "plot(density(data))\n",
    "\n",
    "# QQ plot\n",
    "qqnorm(data)\n",
    "qqline(data, col = 2)\n",
    "\n",
    "# Boxplot (identify outliers)\n",
    "geom_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "Null Hypothesis $H_0$ and Alternative Hypothesis $H_1$\n",
    "- Type 1 Error $\\alpha = P(\\text{rejecting } H_0 | H_0 \\text{ is True})$\n",
    "- Type 2 Error $\\beta= P(\\text{not rejecting } H_0 | H_0 \\text{ is False})$\n",
    "\n",
    "### Power of Test\n",
    "Power of Test = $1 - \\beta$ $\\Rightarrow$ Probability of not commiting a Type 2 error\n",
    "- Sensitive to Sample size: small sample sizes $\\rightarrow$ low power\n",
    "\n",
    "### One-Sample Hypothesis Tests\n",
    "When testing for means, use **z-test** if variance $\\sigma^2$ known. Use **t-test** if variance $\\sigma^2$ unknown.\n",
    "$$\n",
    "z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\\\\n",
    "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
    "$$\n",
    "\n",
    "Get p-value of $H_0$ = `pt(t-value, df=sample_size-1, lower.tail=TRUE/FALSE)`  \n",
    "Reject $H_0$ if p-value < 0.05\n",
    "\n",
    "To test for Proportion:  \n",
    "$\\Rightarrow$ Compute z-statistic for proportion  \n",
    "$$\n",
    "z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\n",
    "$$\n",
    "$\\Rightarrow$ Compute critical value: `qnorm(0.05)`  \n",
    "$\\Rightarrow$ Check if z-statistic < critical value  \n",
    "If z-statistic is **less than** critical value, then reject $H_0$\n",
    "\n",
    "\n",
    "### Two-Sample Hypothesis Tests\n",
    "If testing differences in **Mean**:\n",
    "- If population variances $\\sigma^2$ unknown,  \n",
    "`t.test(outcome ~ group, alternative=\"greater/two.sided/less\", data)`  \n",
    "`t.test(group1, group2)`\n",
    "- If variances known to be equal, set `var.equal=TRUE`  \n",
    "- For Paired test, set `paired=TRUE`\n",
    "\n",
    "If testing differences in **Variance**:\n",
    "- `var.test(y~x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Equality of Variances\n",
    "F-test: `var.test(outcome ~ group)`  \n",
    "Other Tests: (if p-value < 0.05, no equal variance)\n",
    "- Bartlett's Test `bartlett.test()`\n",
    "    - For normally distributed data where there are 2 or more samples\n",
    "- Levene's Test `leveneTest() in library(car)`\n",
    "    - Alternative to Bartlett's test, less sensitive to departures from normality\n",
    "- Fligner-Killeen Test `fligner.test()`\n",
    "    - Non-parametric test that is very robust to departures from normality\n",
    "\n",
    "### ANOVA\n",
    "Compare means of two or more population groups.  \n",
    "`aov_data <- aov(outcome~group, data)`  (group variable should be a factor)  \n",
    "`summary(aov_data)`\n",
    "\n",
    "Assumptions:\n",
    "1. Independence $\\rightarrow$ Randomly and independently obtained (validated if random samples were chosen)\n",
    "2. Normality $\\rightarrow$ Normally distributed (ANOVA is fairly robust to departures from normality)\n",
    "3. Homogeneity $\\rightarrow$ Have equal variances (okay if sample sizes are equal)\n",
    "\n",
    "Can use Welch ANOVA test `welch_anova_test(data, variable~group)` if variances unequal\n",
    "\n",
    "Post-Hoc tests: (Further Analysis)\n",
    "- One-way ANOVA $\\rightarrow$ use `TukeyHSD(aov_data)`\n",
    "- Welch's ANOVA $\\rightarrow$ use `games_howell_test(data, variable~group)`\n",
    "- Control with familywise error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and Estimation\n",
    "### Confidence Interval\n",
    "$$\n",
    "\\text{CI for Mean with Known StDev} = \\bar{x} \\pm z_{\\alpha/2}\\left(\\frac{\\sigma}{\\sqrt{n}}\\right)\\\\\n",
    "\\text{CI for Mean with Unknown StDev} = \\bar{x} \\pm t_{\\alpha/2, n-1}\\left(\\frac{s}{\\sqrt{n}}\\right) \\\\\n",
    "\\text{CI for Mean for Proportion} = \\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
    "$$\n",
    "\n",
    "$z_{\\alpha/2}$ = `qnorm(1-alpha/2)`  \n",
    "$t_{\\alpha/2, n-1}$ = `qt((1-alpha/2), df=n-1)`\n",
    "\n",
    "### Prediction Interval\n",
    "(Need to **check for normality** first)\n",
    "$$\n",
    "\\bar{x} \\pm t_{\\alpha/2, n-1}\\left(s\\sqrt{1 + \\frac{1}{n}}\\right)\n",
    "$$\n",
    "\n",
    "To reduce the sampling error to be within an error of $\\pm E$\n",
    "$$\n",
    "\\text{Sample size for Mean} = n \\geq (z_{\\alpha/2})^2\\frac{\\sigma^2}{E^2} \\\\\n",
    "\\text{Sample size of Proportion} = n \\geq (z_{\\alpha/2})^2\\frac{\\pi(1-\\pi)}{E^2}\n",
    "$$\n",
    "\n",
    "Set $\\pi = 0.5$ if no information is given about the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI for Mean (95% CI)\n",
    "upperCI <- mean(data$Value) - qt(0.025, df=nrow(data)-1)*sd(data$Value)/sqrt(nrow(data))\n",
    "lowerCI <- mean(data$Value) + qt(0.025, df=nrow(data)-1)*sd(data$Value)/sqrt(nrow(data))\n",
    "\n",
    "# CI for Proportion (95% CI)\n",
    "book <- data %>% filter(Product == \"Book\")\n",
    "bk50 <- book %>% filter(Amount > 50)\n",
    "propbk50 <- nrow(bk50) / nrow(book)\n",
    "upperCI <- propbk50 - qnorm(0.025) * sqrt(propbk50*(1-propbk50)/nrow(book))\n",
    "lowerCI <- propbk50 + qnorm(0.025) * sqrt(propbk50*(1-propbk50)/nrow(book))\n",
    "\n",
    "# 95% Prediction Interval (Need to check for Normality)\n",
    "mean.val <- mean(data$Value)\n",
    "sd.val <- sd(data$Value)\n",
    "\n",
    "upperPI <- mean.val + (qt(0.975, df=nrow(data)-1))*sd.val*sqrt(1+1/nrow(data))\n",
    "lowerPI <- mean.val - (qt(0.975, df=nrow(data)-1))*sd.val*sqrt(1+1/nrow(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Normality\n",
    "- Density Plot\n",
    "- Histogram\n",
    "- QQ plot\n",
    "- Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plot\n",
    "ggplot(data, aes(x=col)) +\n",
    "    geom_density()\n",
    "\n",
    "# QQ plot\n",
    "qqnorm(data$col)\n",
    "qqline(data$col)\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "# if W close to 1, p-value > 0.05 => implies data does not deviate from normality\n",
    "shapiro.test(data$col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to Normal\n",
    "Transform data using `transformTukey`, to get normally distributed data  \n",
    "Need to convert results back for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Normal\n",
    "library(rcompanion)\n",
    "transformTukey(data$col) #note the lambda output\n",
    "\n",
    "#Revert data back\n",
    "upperResults <- transformedUpper ^ (1/lambda)\n",
    "lowerResults <- transformedLower ^ (1/lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predictive Analytics\n",
    "**What will happen?**  \n",
    "Predict future by examining historical data, detecting patterns or relationships in these data, then extrapolating these relationships forward in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear / Logistic Regression\n",
    "\n",
    "Steps to run prior to fitting a Regression Model:\n",
    "1. Obtain descriptive statistics on all variables\n",
    "    - Visualize key variables $\\rightarrow$ using histograms, bar charts, line charts, scatterplots\n",
    "    - Detect any outliers\n",
    "2. Deal with legitmate outliers in Step 1\n",
    "3. Check for any intercorrelations between independent variables\n",
    "    - Generate correlation matrix\n",
    "    - Remove/reduce multicollinearity by centered data, standardizing the independent variables, removing the variables\n",
    "\n",
    "### Linear Regression / Ordinary Least Squares (OLS)\n",
    "Minimises sum of squares of residuals\n",
    "$$\n",
    "Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + \\dots\n",
    "$$\n",
    "Where:\n",
    "- $b_0$: Mean value of Y when X is zero\n",
    "- $b_1$: A one unit change in $X_1$ results in a $b_1$ unit change in Y, when all other variables are held constant.\n",
    "- $b_2$: A one unit change in $X_2$ results in a $b_2$ unit change in Y, when all other variables are held constant.\n",
    "- $b_3$ (Assuming $X_3$ is categorical): Y is $b_3$ higher when $X_3$ is true, as compared to the reference group\n",
    "- R-squared: measures the proportion of the variance explained by the model\n",
    "- p-value: shows the significance of the regressions model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model \n",
    "fit1 <- lm(y~x1 + x2 + ..., df1)\n",
    "summary(fit1)\n",
    "predict(fit1)\n",
    "residuals(fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "Predicting the log-odds of an event occuring\n",
    "$$\n",
    "logit(p) = log \\frac{p}{1-p} = b_0 + b_1X_1 + b_2X_2\n",
    "$$\n",
    "Where:\n",
    "- $b_0$: Log-odds when $X_1$ and $X_2$ are both zero\n",
    "- $b_1$: Expected increase in log-odds of event when $X_1$ becomes 1, holding $X_2$ constant\n",
    "- $b_2$: Expected increase in log-odds of event per unit-increase of $X_2$, holding $X_1$ constant / or multiples the odds by $exp(b_2)$\n",
    "- Predicted odds = $exp(b_0 + b_1X_1 + b_2X_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "fit_log1 <- glm(y ~ x1 + x2 + ..., family=\"binomial\", dataset)\n",
    "predict(fit_log1, newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "- Trends: Gradual upward or downward movement of time series over time\n",
    "- Seasonality: Effect that occurs/repeats at a fixed interval\n",
    "- Cyclical Effects: Longer-term effects that do not have a fixed interval/length\n",
    "- Stationary: Statistical properties (mean, variance) of the time-series does not change over time\n",
    "\n",
    "**Moving Average model:**\n",
    "$$\n",
    "Window_t = \\frac{1}{n}\\left(Y_t + Y_{t-1} + \\dots + Y_{t-(n-1)}\\right)\n",
    "$$\n",
    "\n",
    "**Exponential Smoothing model:**  \n",
    "$\\Rightarrow$ Gives more weight to recent values & uses all past data\n",
    "$$\n",
    "\\hat{Y}_{t+1} = \\alpha Y_t + (1-\\alpha)\\hat{Y}_t\n",
    "$$\n",
    "\n",
    "### Regression-based Forecasting\n",
    "- Leading variable: $X$ is leading variable if changes in $X_t$ precedes changes in $Y_t$\n",
    "- Lagging variable: $Y$ is lagging variable if it follows movements of other variables\n",
    "- Coincident: both tend to co-vary at the same time\n",
    "\n",
    "Interaction terms: Tests if the effect of one IV on the DV changes depending on the value of another IV\n",
    "`lm(y ~ m*x, df) == lm(y ~ x + m + m*x, df)`  \n",
    "Coefficient interpretation: Difference between the slopes of the groups\n",
    "\n",
    "### Model Selection\n",
    "**ANOVA:** Use ANOVA to compare if explanatory power of full model is significantly better than the restricted model.  \n",
    "`anova(m_restricted, m_full)`\n",
    "\n",
    "**Forward/Backwards Stepwise Regression:**  \n",
    "`step(m_full, direction = \"forward/backward/both\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Average model\n",
    "library(TTR)\n",
    "df$sma2 = SMA(df$y, n=2) # taking window size of 2\n",
    "df$sma_predict = dplyr::lag(df$sma2, 1) # lagging by 1\n",
    "\n",
    "# Exponential Smoothing model\n",
    "# Single (no trend/seasonality)\n",
    "HoltWinters(x, beta=FALSE, gamma=FALSE)\n",
    "\n",
    "# Double (Trend & no seasonality)\n",
    "HoltWinters(x, gamma=FALSE)\n",
    "\n",
    "# Triple (Trend & seasonality)\n",
    "HoltWinters(x)\n",
    "predict(fit, n.ahead = k)\n",
    "\n",
    "# Data Prep for Timeseries\n",
    "library(tsbox)\n",
    "ts_df(timeseries) #convert from Timeseries object to Dataframe\n",
    "ts_ts(dataframe) #convert from Dataframe object to Timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining\n",
    "### Data Exploration\n",
    "Use a Pair Plot to visualise all the variables distribution and their correlation coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(psych)\n",
    "pairs.panels(data, lm=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction (Dimensionality Reduction)\n",
    "**Principal Component Analysis (PCA)**  \n",
    "$\\Rightarrow$ Finds a set of features (\"Principal Components\") that explains the most variance in the data  \n",
    "`pca <- prcomp(df, center=TRUE, scale=TRUE)`  \n",
    "`summary(pca)`  \n",
    "$\\Rightarrow$ Important to have `scale=T` to standardize all the variables before running PCA\n",
    "\n",
    "**k-means Clustering**  \n",
    "`kmeans(df, num_of_clusters, nstart=n)`  \n",
    "$\\Rightarrow$ `nstart` argument, R will repeat kmeans with `n` different initialisations and return the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca <- prcomp(df, center=T, scale=T)\n",
    "summary(pca) # examining the output\n",
    "pca$rotation[, 1:2] # examining loadings (distribution) on first 2 PCs\n",
    "df$pc1 <- pca$x[, \"PC1\"] # extracting first PC\n",
    "lm(col ~ pc1, df) # using the PC in linear model\n",
    "\n",
    "## Plotting PCA\n",
    "library(devtools)\n",
    "install_github(\"vqv/ggbiplot\")\n",
    "library(ggbiplot)\n",
    "ggbiplot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means Clustering\n",
    "km_obj <- kmeans(df, num_of_clusters, nstart=10)\n",
    "## to find optimal number of clusters (identify Elbow point)\n",
    "wss <- rep(NA, 10)\n",
    "for (k in c(1:10)) {\n",
    "    wss[k] <- kmeans(df, k, nstart=10)$tot.withinss\n",
    "}\n",
    "plot(wss, type=\"b\", xlab=\"Number of clusters\", ylab=\"Total within-cluster sum of squares\")\n",
    "\n",
    "## Plotting the Clusters\n",
    "fviz_cluster(km_obj, df)\n",
    "## Get the centers of each cluster\n",
    "km_obj$centers\n",
    "\n",
    "# Regularisation\n",
    "## alpha=0 (Lasso) / 1 (Ridge)\n",
    "df_X = as.matrix(df[, X_COLS])\n",
    "df_Y = as.matrix(df[, Y_COL])\n",
    "lm_reg1 <- cv.glmnet(df_train_X, df_train_Y, alpha=0/1, family=\"gaussian\")\n",
    "coef(lm_reg1, s=\"lambda.min\") # returns coeffs at the lambda that gives minimum MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classification  \n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}\\\\\n",
    "\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}\\\\\n",
    "\\text{F1-Score} = \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision + Recall}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "table(data$col1, data$col2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prescriptive Analytics\n",
    "**How to make it happen?**  \n",
    "Identify the best alternatives to minimize or maximize some objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Optimization (LP Relaxation)\n",
    "Steps:  \n",
    "1. Identify the Objective Function & Decision Variables\n",
    "2. Identify the Constraints\n",
    "3. Write down the Optimisation model\n",
    "4. Use R to solve\n",
    "5. Conduct Sensitivity Analysis\n",
    "6. Interpret Results. Write a Recommendation.\n",
    "\n",
    "### Sensitivity Analysis\n",
    "**On Objective Function Coefficients**  \n",
    "Shows the upper/lower bounds for the coefficients of variables, whereby the solution will stay optimal\n",
    "\n",
    "**On Constraint Values**  \n",
    "Shadow prices $\\rightarrow$ how will increasing/decreasing constraint values affect the optimal solution\n",
    "\n",
    "## Integer Optimisation\n",
    "Constraint that the solution has to be an integer\n",
    "\n",
    "**Logical Constraints:**\n",
    "- If A, then B $\\Rightarrow$ $B - A \\geq 0 $\n",
    "- If not A, then B $\\Rightarrow$ $A + B \\geq 1 $\n",
    "- If A, then not B $\\Rightarrow$ $A + B \\leq 1 $\n",
    "- At most one of A and B $\\Rightarrow$ $A + B \\leq 1 $\n",
    "- If A, then B and C $\\Rightarrow$ $B + C - 2A \\geq 0 $\n",
    "- If A and B, then C $\\Rightarrow$ $A + B - C \\leq 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear/Integer Optimisation\n",
    "library(lpSolve)\n",
    "objective.fn <- c(2, 3, 5, 6, 8) # Profit = 2*X_1 + 3*X_2 + 5*X_3 + 6*X_4 + 8*X_5\n",
    "const.mat <- matrix(c(rep(0,0), 1, -1, rep(0, 3), #Constraint1\n",
    "                     rep(0,1), 1, -1, rep(0, 2), #Constraint2\n",
    "                     rep(0,2), 1, -1, rep(0, 1) #Constraint3\n",
    "                     ), ncol= 5, byrow=TRUE)\n",
    "single_contraint <- (rep(0, 5), 1, -1, rep(0, 3)) #to write sparse constraints quickly\n",
    "const.dir <- c(\"<=\", \"=\", rep(\">=\", 3)) # \">=, =, <=\"\n",
    "const.rhs <- c(100, 200, rep(50, 3)) # Right-side value of constraint\n",
    "# lpSolve assumes non-negativity constraints -> no need to explicitly declare\n",
    "\n",
    "#Solve Linear Optimisation\n",
    "lp.soln <- lp(\"max/min\", objective.fn, const.mat, const.dir, const.rhs, compute.sens=TRUE)\n",
    "lp.soln # optimal objective function value\n",
    "lp.soln$solution # decision variables values\n",
    "\n",
    "##Sensitivity Analysis\n",
    "### Bounds before the optimal solution will change\n",
    "lp.soln$sens.coef.to # upper bounds on the coefficients \n",
    "lp.soln$sens.coef.from # lower bounds on the coefficients\n",
    "\n",
    "##Shadow Prices\n",
    "### Shows the increase/decrease in final objective value if constraint is increased by one unit\n",
    "### Last two values shown are for the non-negativity constraints\n",
    "lp.soln$duals\n",
    "\n",
    "#Solve Integer Optimisation\n",
    "int.soln <- lp(\"max/min\", objective.fn, const.mat, const.dir, const.rhs, \n",
    "   int.vec = c(1,2,3), #define which variables have to be integers\n",
    "   binary.vec = c(4:6), #define which variables are binary (0 or 1)\n",
    "   compute.sens=FALSE) #sensitivity analysis doesnt work for integer constraint\n",
    "int.soln\n",
    "int.soln$solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
