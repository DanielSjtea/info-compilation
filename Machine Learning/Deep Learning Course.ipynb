{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d04bb2ee",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa065b7",
   "metadata": {},
   "source": [
    "## Initialisation of Weights\n",
    "- The weights $W^{[l]}$ should be initialized randomly to break symmetry\n",
    "- Do not initialize weights to be too large, else the learning rate will be very slow\n",
    "    - When using Relu/Sigmoid activation functions, the gradient at very large/very small values is close to 0, hence learning rate becomes very slow\n",
    "    - Prevents vanishing/exploding gradients\n",
    "    - Recommended Initialization Methods:\n",
    "        - **Xavier Initialization**: Scaling factor of `sqrt(1./layers_dims[l-1]`\n",
    "        - **He Initialization**: Scaling factor of `sqrt(2./layers_dims[l-1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd2ef7f",
   "metadata": {},
   "source": [
    "## Bias and Variance\n",
    "Large difference in **Bayes error and train set error** $\\rightarrow$ High Bias (Underfitting)   \n",
    "Large difference in **train set and validation set error** $\\rightarrow$ High Variance (Overfitting)  \n",
    "Large difference in **validation set and test set error** $\\rightarrow$ Data Mismatch (Distribution of data in validation set not same as test set)\n",
    "\n",
    "Fixes for High Bias (Underfitting):\n",
    "- Train a bigger neural network\n",
    "- Train longer\n",
    "\n",
    "Fixes for High Variance (Overfitting):\n",
    "- Use more data\n",
    "- Regularization\n",
    "    - L1/L2 Regularization\n",
    "        - L1 Loss produces a sparse weights result\n",
    "        - L2 Loss more commonly used\n",
    "    - Dropout\n",
    "        - Intuition: Can't rely on any one feature, so have to spread out weights\n",
    "    - Data Augmentation\n",
    "    - Weight Decay\n",
    "    - Early Stopping\n",
    "        - Issue: Affects two areas, both optimizes cost function and regularises\n",
    "\n",
    "Fixes for Data Mismatch:\n",
    "- Ensure data distribution of validation set is same as test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5ca87",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "- `sigmoid` function: range from 0 to 1, use for binary classification output layer\n",
    "- `tanh` function: range from -1 to +1, almost always better than the sigmoid function due to mean 0 (data is centered)\n",
    "- `Relu` function: `max(0, z)` **recommended**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b061ae",
   "metadata": {},
   "source": [
    "## Normalizing\n",
    "### Normalizing Inputs\n",
    "- Helps to **speed up training** as it affects the rate of learning for gradient descent\n",
    "    - Has a symmetrical descent, instead of an elongated descent direction\n",
    "\n",
    "### Batch Normalization\n",
    "Normalizes all the output activations $a^l$ to train $w^{l+1}$ and $b^{l+1}$ faster\n",
    "\n",
    "Implementation:\n",
    " - Given some intermediate values in layer L of NN: $z^{(1)}, \\dots, z^{(m)}$\n",
    " - $\\mu = \\frac{1}{m}\\sum_i z^{(i)}$\n",
    " - $\\sigma^2 = \\frac{1}{m}\\sum_i (z^{(i)} - \\mu)^2$\n",
    " - $z^{(i)}_{norm} = \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}}$\n",
    " - $\\tilde{z}^{(i)} = \\gamma z^{(i)}_{norm} + \\beta$\n",
    " \n",
    "Last step is because you may not want all your hidden unit values to have mean 0 and variance 1 (e.g. having a sigmoid function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ebe066",
   "metadata": {},
   "source": [
    "### Adam Optimizer\n",
    "The **Adam** optimizer combines both the concepts of using **momentum** as well as **RMSprop**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e956491",
   "metadata": {},
   "source": [
    "# ML Strategy\n",
    "\n",
    "Approach for any problem: Quickly produce a first model and see what errors it is making, then iterate on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23314ed3",
   "metadata": {},
   "source": [
    "## Setting Up Your Goal\n",
    "\n",
    "Evaluate your models using a **Single Number Evaluation Metric**.\n",
    "\n",
    "Can use human level performance as the Bayes error $\\rightarrow$ the smallest unavoidable bias attainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d7a9c",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Manually look at a subset of the incorrectly labelled examples in the validation/dev set. Label each example with the appropriate reason for mismatch.\n",
    "\n",
    "Provides a way to quickly approximate what errors to focus on, provides a benchmark for the \"maximum\" improvement to the model if a certain error is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fbaf3",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning from A $\\rightarrow$ B\n",
    "\n",
    "Reasons for Use:\n",
    "- Task A and B have the same input $x$\n",
    "- Have a lot more data for Task A than Task B\n",
    "- Low level features from A could be helpful for learning B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be200f97",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4a36c",
   "metadata": {},
   "source": [
    "## Streaming the Data\n",
    "\n",
    "Here you should take note of an important extra step that's been added to the batch training process: \n",
    "\n",
    "- `tf.Data.dataset = dataset.prefetch(8)` \n",
    "\n",
    "What this does is prevent a memory bottleneck that can occur when reading from disk. `prefetch()` sets aside some data and keeps it ready for when it's needed. It does this by creating a source dataset from your input data, applying a transformation to preprocess the data, then iterating over the dataset the specified number of elements at a time. This works because the iteration is streaming, so the data doesn't need to fit into the memory. \n",
    "\n",
    "`X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)` # <<< extra step    \n",
    "`Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8)` # loads memory faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c01343-d720-49ba-b2f0-160134efd84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
