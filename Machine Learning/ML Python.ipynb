{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "\n",
    "# Supervised ML Methods\n",
    "\n",
    "## Regression\n",
    "- Linear Regression (Feature Engineering eg. Polynomial Values)\n",
    "- Subset Selection (Best subset, forward/backwards stepwise)\n",
    "- Shrinkage Methods\n",
    " - Ridge Regression\n",
    " - Lasso\n",
    " - SCAD\n",
    "\n",
    "## Classification\n",
    "- Logistic Regression\n",
    "- LDA / QDA\n",
    "\n",
    "## Both\n",
    "- K-Nearest Neighbours\n",
    "- Decision Trees\n",
    " - Regression Trees\n",
    " - Classification Trees\n",
    " - Bagging & Bootstrap\n",
    " - Random Forest\n",
    " - Boosting\n",
    "- Neural Networks\n",
    "\n",
    "# Unsupervised ML Methods\n",
    "- K-Means Clustering\n",
    "- Hierarchical Clustering\n",
    "- PCA\n",
    "\n",
    "# Dimensionality Reduction \n",
    "- PCR\n",
    "- PLS\n",
    "- SIR\n",
    "- SIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading general libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.arange(10).reshape((10, )), range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "# 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#sklearn LinearRegression expects a 2d array as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using X_train[:, None] to convert a 1d array to a 2d array\n",
    "lin_reg = LinearRegression().fit(X_train[:,None], y_train)\n",
    "y_test_pred = lin_reg.predict(X_test[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Feature Engineering\n",
    "### Polynomial Features: $x^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_features(x_array, p):\n",
    "    covariates = []\n",
    "    for k in np.arange(1,p+1):\n",
    "        covariates.append( x_array**k )\n",
    "    print(covariates)\n",
    "    X_poly = np.column_stack(covariates)\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Subset Selection (Use R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Shrinkage Methods\n",
    "**Reminder**: Standardize predictors to have mean 0 and standard deviation 1 before using any shrinkage method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Ridge Regression\n",
    "Uses the $l_2$ loss, minimizes:\n",
    "$$\n",
    "\\frac{RSS}{2} + \\alpha\\sum^{p}_{j=1}\\beta^2_j\n",
    "$$\n",
    "- Decreases variance but increases bias\n",
    "- Performs better when the response is a function of **many predictors**, all with coefficients of roughly equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "clf = Ridge(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Lasso\n",
    "Uses the $l_1$ loss, minimizes:\n",
    "$$\n",
    "\\frac{RSS}{2} + \\alpha\\sum^{p}_{j=1}|\\beta|\n",
    "$$\n",
    "- Performs **variable selection**\n",
    "- Produces simpler and more interpretable models that involve only a subset of the predictors\n",
    "- Performs better when a **small number** of predictors have substantial coefficients, and the rest have coefficients that are very small or equal zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "clf = Lasso(alpha=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 SCAD\n",
    "- Small coefficient are set to zero, moderate coefficients shrunk towards zero while retaining large coefficients as they are\n",
    "- Produces a sparse solution and approx unbiased estimates for large coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 K-Nearest Neighbor (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Logistic Regression\n",
    "Can be used for either binomial classification or multinomial classification\n",
    "\n",
    "**(In R)** Select the best logistic model (with a reduced number of variables) using stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression.fit(X_train, y_train)\n",
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 LDA/QDA\n",
    "- LDA assumes normality of predictors and common covariance, hence is more stable than logistic regression\n",
    "- QDA is the compromise between KNN, LDA, and logreg. Can accurately model a wider range of problems than linear methods, but is not as flexible as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
    "\n",
    "from sklearn.qda import QDA\n",
    "clf = QDA().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 K-Nearest Neighbour (Classifier)\n",
    "- Makes no assumptions about the shape of decision boundary\n",
    "- Cannot be used in high-dimensions or to identify important predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees\n",
    "See also LightGBM: https://lightgbm.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "# Plot the tree\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain graph of accuracies across different depths\n",
    "max_depth = []\n",
    "acc_gini = []\n",
    "acc_entropy = []\n",
    "for i in range(2,50):\n",
    "    dtree = DecisionTreeClassifier(random_state=8,criterion='gini', max_leaf_nodes=i)\n",
    "    dtree.fit(X_train, y_train)\n",
    "    pred = dtree.predict(X_test)\n",
    "    acc_gini.append(accuracy_score(y_test, pred))\n",
    "    ###\n",
    "    dtree = DecisionTreeClassifier(criterion='entropy', max_depth=i)\n",
    "    dtree.fit(X_train, y_train)\n",
    "    pred = dtree.predict(X_test)\n",
    "    acc_entropy.append(accuracy_score(y_test, pred))\n",
    "    ####\n",
    "    max_depth.append(i)\n",
    "d = pd.DataFrame({'acc_gini':pd.Series(acc_gini), \n",
    "    'acc_entropy':pd.Series(acc_entropy),\n",
    "    'max_depth':pd.Series(max_depth)})\n",
    "# visualizing changes in parameters\n",
    "plt.plot('max_depth','acc_gini', data=d, label='gini')\n",
    "plt.plot('max_depth','acc_entropy', data=d, label='entropy')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "print(\"Best max_depth for gini:\", d.sort_values('acc_gini',ascending=False).iloc[0,2])\n",
    "print(\"Best max_depth for entropy:\", d.sort_values('acc_entropy',ascending=False).iloc[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Bagging & Bootstrap\n",
    "- Bagging builds many prediction models and takes the average resulting predictions\n",
    "- **Reduces variance**, as decision trees have high variance\n",
    "- Bootstrap creates many training sets from a single dataset\n",
    "\n",
    "\n",
    "- Resulting model can be difficult to interpret, hence use summary of importance of predictor using RSS (regression) or Gini index (classification) $\\Rightarrow$ large value indicates an important predictor\n",
    "\n",
    "### 3.4.1 Out-of-Bag Error Estimation\n",
    "- OOB error is a valid estimation of test error of bagged model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Random Forests\n",
    "- **Decorrelates** the trees by only considering a random sample of *m* predictors at each split instead of all the predictors\n",
    "- *m* set at $\\frac{p}{3}$ (regression) or $\\sqrt{p}$ (classification)\n",
    "\n",
    "\n",
    "- Logic of Random Forests: if there is a very strong predictor, for bagging, most of the trees will use that predictor for the top split and all the bagged trees will look similar, and hence highly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Boosting\n",
    "- Trees are grown **sequentially** using information from previously grown trees\n",
    "- Very time and resource heavy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Networks\n",
    "- Can model non-linearity very well\n",
    "- Difficult to interpret the resulting model\n",
    "\n",
    "**Providers**\n",
    "- Tensorflow\n",
    " - Keras\n",
    "- Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Regression and Classification NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Convolutional NN\n",
    "- Works well for images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 LSTM and GRU\n",
    "- Works well for financial and stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal Component Regression (PCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partial Least Squares (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sure Independence Screening (SIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sliced Inverse Regression (SIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
