{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad54c956-d53a-46c2-b2e6-99d34e2b5a23",
   "metadata": {},
   "source": [
    "# Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b033c08-472a-40c6-bf47-3a38fb63be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")\n",
    "\n",
    "adult_census.head()\n",
    "\n",
    "# Getting counts for the target column\n",
    "# Check if there is a class imbalance\n",
    "target_column = 'class'\n",
    "data = adult_census.drop(columns=[target_column])\n",
    "target = adult_census[target_column]\n",
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb909a-af81-4e4e-aa40-74fac78a7e61",
   "metadata": {},
   "source": [
    "## Visualising the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6709d4b-5a92-408b-9736-4340782a3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = adult_census.hist(figsize=(20, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd410c1-5c49-4147-828b-e6982fdd9e70",
   "metadata": {},
   "source": [
    "Using a `pairplot` can quickly visualise the distribution of the variables (on the diagonals) and the correlation between the variables (on the off-diagonals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3342c-a7b0-447d-9655-eb9a46cce248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# We will plot a subset of the data to keep the plot readable and make the\n",
    "# plotting faster\n",
    "n_samples_to_plot = 5000\n",
    "columns = ['age', 'education-num', 'hours-per-week']\n",
    "_ = sns.pairplot(data=adult_census[:n_samples_to_plot], vars=columns,\n",
    "                 hue=target_column, plot_kws={'alpha': 0.2},\n",
    "                 height=3, diag_kind='hist', diag_kws={'bins': 30})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aab56e-63e9-46ce-ac05-fb896631f9c7",
   "metadata": {},
   "source": [
    "# Diagram Mode for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1633c-819a-4e3c-a2c0-c5683b45a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682e437-977a-4d66-adad-e706391e7441",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "[Reasons for Feature Scaling:](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)\n",
    "- Models that rely on the distance between a pair of samples, for instance k-nearest neighbors, should be trained on normalized features to make each feature contribute approximately equally to the distance computations.\n",
    "- Many models such as logistic regression use a numerical solver (based on gradient descent) to find their optimal parameters. This solver converges faster when the features are scaled.\n",
    "\n",
    "- Models that rely on the distance between a pair of samples, for instance k-nearest neighbors, should be trained on normalized features to make each feature contribute approximately equally to the distance computations.\n",
    "- Many models such as logistic regression use a numerical solver (based on gradient descent) to find their optimal parameters. This solver converges faster when the features are scaled.\n",
    "- Predictors using Euclidean distance, for instance k-nearest-neighbors, should have normalized features so that each one contributes equally to the distance computation;\n",
    "- predictors using gradient-descent based algorithms, for instance logistic regression, to find optimal parameters work better and faster;\n",
    "- predictors using regularization, for instance logistic regression, require normalized features to properly apply the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc4728",
   "metadata": {},
   "source": [
    "# Handling Categorical/Numerical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c10f5",
   "metadata": {},
   "source": [
    "## Strategies to Encode Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e95c1",
   "metadata": {},
   "source": [
    "### Encoding Ordinal Categories\n",
    "The `OrdinalEncoder` will encode each category with a different number.\n",
    "\n",
    "**Cons:** If a categorical variable does not carry any meaningful order information, then this encoding might be misleading to downstream statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns = data.select_dtypes([\"object\"])\n",
    "numerical_columns = data.select_dtypes([\"integer\", \"float\"])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "cols_encoded = encoder.fit_transform(categorical_columns)\n",
    "\n",
    "# To check the mapping between each category and the encoding\n",
    "cols_encoded.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607d37c",
   "metadata": {},
   "source": [
    "### Encoding Nominal Categories (without assuming any order)\n",
    "`OneHotEncoder` is an alternative encoder that prevents the downstreams models to make a false assumption about the ordering of categories. For a given feature, it will create as many new columns as there are possible categories. For a given sample, the value of the column corresponding to the category will be set to `1` while allthe columns of the other categories will be set to `0`.\n",
    "\n",
    "**Cons:** Creates many additional columns to denote each possible category value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f46cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "cols_encoded = encoder.fit_transform(categorical_columns)\n",
    "\n",
    "feature_names = encoder.get_feature_names(input_features=[\"categorical_variables\"])\n",
    "data_encoded = pd.DataFrame(cols_encoded, columns=feature_names)\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76eafb",
   "metadata": {},
   "source": [
    "### Choosing an Encoding strategy\n",
    "Choosing an encoding strategy will depend on the underlying models and the type of categories (i.e. ordinal vs. nominal).\n",
    "\n",
    "Indeed, using an `OrdinalEncoder` will output ordinal categories. It means that there is an order in the resulting categories (e.g. 0 > 1 > 2). The impact of violating this ordering assumption is really dependent on the downstream models. Linear models will be impacted by misordered categories while tree-based models will not be.\n",
    "\n",
    "Thus, in general `OneHotEncoder` is the encoding strategy used when the downstream models are **linear models** while `OrdinalEncoder` is used with **tree-based models**.\n",
    "\n",
    "You still can use an `OrdinalEncoder` with linear models but you need to be sure that:\n",
    "\n",
    "- the original categories (before encoding) have an ordering\n",
    "- the encoded categories follow the same ordering as the original categories. The next exercise highlight the issue of misusing `OrdinalEncoder` with a linear model.\n",
    "\n",
    "Also, there is no need to use an `OneHotEncoder` even if the original categories do not have an given order with tree-based model. It will be the purpose of the final exercise of this sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb345fb",
   "metadata": {},
   "source": [
    "### Handling Unknown Categories\n",
    "In scikit-learn, there are two solutions to bypass this issue:\n",
    "\n",
    "- list all the possible categories and provide it to the encoder via the keyword argument `categories`\n",
    "- use the parameter `handle_unknown`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0249352-8166-41ed-b43a-9c9aa9edcaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"), \n",
    "    LogisticRegression(max_iter=500)\n",
    ")\n",
    "\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c5029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(model, data_categorical, target)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9484db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv_results[\"test_score\"]\n",
    "print(f\"The accuracy is: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67152f60",
   "metadata": {},
   "source": [
    "### Dispatch columns to a specific processor\n",
    "\n",
    "Scikit-learn provides a `ColumnTransformer` class which will send specific\n",
    "columns to a specific transformer, making it easy to fit a single predictive\n",
    "model on a dataset that combines both kinds of variables together\n",
    "(heterogeneously typed tabular data).\n",
    "\n",
    "We first define the columns depending on their data type:\n",
    "\n",
    "* **one-hot encoding** will be applied to categorical columns. Besides, we\n",
    "  use `handle_unknown=\"ignore\"` to solve the potential issues due to rare\n",
    "  categories.\n",
    "* **numerical scaling** numerical features which will be standardized.\n",
    "\n",
    "Now, we create our `ColumnTransfomer` by specifying three values:\n",
    "the preprocessor name, the transformer, and the columns.\n",
    "\n",
    "A `ColumnTransformer` does the following:\n",
    "\n",
    "* It **splits the columns** of the original dataset based on the column names\n",
    "  or indices provided. We will obtain as many subsets as the number of\n",
    "  transformers passed into the `ColumnTransformer`.\n",
    "* It **transforms each subsets**. A specific transformer is applied to\n",
    "  each subset: it will internally call `fit_transform` or `transform`. The\n",
    "  output of this step is a set of transformed datasets.\n",
    "* It then **concatenate the transformed datasets** into a single dataset.\n",
    "\n",
    "The important thing is that `ColumnTransformer` is like any other\n",
    "scikit-learn transformer. In particular it can be combined with a classifier\n",
    "in a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ef5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('one-hot-encoder', categorical_preprocessor, selector(dtype_include=object)), #Only need to provide the column names\n",
    "    ('standard-scaler', numerical_preprocessor, selector(dtype_exclude=object))],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc059",
   "metadata": {},
   "source": [
    "# Selecting the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13427b6-3326-4b49-a14f-4fc9e3fecbe6",
   "metadata": {},
   "source": [
    "# Overfitting/Underfitting\n",
    "- High Bias == Underfitting\n",
    "    - systematic prediction errors\n",
    "    - the model prefers to ignore some aspects of the data\n",
    "    - mispecified models\n",
    "- High Variance == Overfitting\n",
    "    - prediction errors without obvious structure\n",
    "    - small change in the training set, large change in model\n",
    "    - unstable models\n",
    "    \n",
    "**Overfitting** is caused by the **limited size** of the training set, the **noise** in the data, and the **high flexibility** of common machine learning models.\n",
    "\n",
    "**Underfitting** happens when the learnt prediction functions suffers from **systematic errors**. This can be caused by a choice of model family and parameters, which leads to a **lack of flexibility** to capture the repeatable structure of the true data generating process.\n",
    "\n",
    "Increasing the training set size will **decrease overfitting** but can also cause an **increase of underfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31fe1b2-43da-495d-b295-24744868d131",
   "metadata": {},
   "source": [
    "## Validation Curve\n",
    "Some model hyperparameters are usually the key to go from a model that\n",
    "underfits to a model that overfits, hopefully going through a region were we\n",
    "can get a good balance between the two. We can acquire knowledge by plotting\n",
    "a curve called the validation curve. This curve can also be applied to the\n",
    "above experiment and varies the value of a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f36d4-bcfc-46ee-93ad-2ad5c755dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca385d-facd-4046-9803-12379135dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(regressor, data, target,\n",
    "                            cv=cv,\n",
    "                            return_train_score=True, n_jobs=-1)\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e1056d-f2af-4b53-aa97-cd4f26346c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    pipeline, data, target, param_name=\"KNN__n_neighbors\", param_range=param_range,\n",
    "    cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fafdb-1cf6-4308-83ad-fa43962b6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(param_range, train_scores.mean(axis=1), label=\"Training score\")\n",
    "plt.plot(param_range, test_scores.mean(axis=1), label=\"Testing score\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Maximum depth of decision tree\")\n",
    "plt.ylabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Validation curve for decision tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eeb766-47ee-4e8c-9073-7e2c5760c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the variance in train/test errors\n",
    "plt.errorbar(param_range, train_scores.mean(axis=1),\n",
    "             yerr=train_scores.std(axis=1), label='Training score')\n",
    "plt.errorbar(param_range, test_scores.mean(axis=1),\n",
    "             yerr=test_scores.std(axis=1), label='Testing score')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Maximum depth of decision tree\")\n",
    "plt.ylabel(\"Mean absolute error (k$)\")\n",
    "_ = plt.title(\"Validation curve for decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f3da6-3fe0-4ccb-b7f0-4dc7a078b13d",
   "metadata": {},
   "source": [
    "### Checking the size of Train Set to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4bca0-fef0-4728-8fe9-ed613b650741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "# Write your code here.\n",
    "train_sizes = np.linspace(0.1, 1, num=10)\n",
    "results = learning_curve(\n",
    "    pipeline, data, target, train_sizes=train_sizes, cv=cv, n_jobs=-1\n",
    ")\n",
    "train_size, train_scores, test_scores = results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a1d32-b0cc-44c4-ac59-a09e17830180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(train_size, train_scores.mean(axis=1),\n",
    "             yerr=train_scores.std(axis=1), label='Training error')\n",
    "plt.errorbar(train_size, test_scores.mean(axis=1),\n",
    "             yerr=test_scores.std(axis=1), label='Testing error')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Number of samples in the training set\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "_ = plt.title(\"Learning curve for support vector machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b610c-945a-4f2c-8f64-66f3f185187c",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bfe00-f8d1-4e73-ad45-d858312fd56f",
   "metadata": {},
   "source": [
    "## Manual Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a151a17-3d5c-47a2-b462-ef34f2b90006",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.01, 0.1, 1, 10]\n",
    "max_leaf_nodes = [3, 10, 30]\n",
    "model.get_params()\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "learning_rate = [0.05, 0.1, 0.5, 1, 5]\n",
    "max_leaf_nodes = [3, 10, 30, 100]\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "for lr in learning_rate:\n",
    "    for mln in max_leaf_nodes:\n",
    "        print(f\"Evaluating model with learning rate {lr:.3f} and max leaf nodes {mln}...\", end=\"\")\n",
    "        model.set_params(\n",
    "            classifier__learning_rate = lr,\n",
    "            classifier__max_leaf_nodes = mln\n",
    "        )\n",
    "        scores = cross_val_score(model, data_train, target_train, cv=2)\n",
    "        mean_score = scores.mean()\n",
    "        print(f\"score: {mean_score:.3f}\")\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = {\"learning-rate\":lr,\n",
    "                          \"max leaf nodes\":mln}\n",
    "            print(f\"Found new best model with score {best_score:.3f}!\")\n",
    "\n",
    "print(f\"The best accuracy obtained is {best_score:.3f}\")\n",
    "print(f\"The best parameters found are:\\n {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81a11-3188-4a30-aa75-ad783408cebf",
   "metadata": {},
   "source": [
    "## Automated using grid-search\n",
    "The `GridSearchCV` estimator takes a `param_grid` parameter which defines\n",
    "all hyperparameters and their associated values. The grid-search will be in\n",
    "charge of creating all possible combinations and test them.\n",
    "\n",
    "The number of combinations will be equal to the product of the\n",
    "number of values to explore for each parameter (e.g. in our example 4 x 4\n",
    "combinations). Thus, adding new parameters with their associated values to be\n",
    "explored become rapidly computationally expensive.\n",
    "\n",
    "Once the grid-search is fitted, it can be used as any other predictor by\n",
    "calling `predict` and `predict_proba`. Internally, it will use the model with\n",
    "the best parameters found during `fit`.\n",
    "\n",
    "**Cons:** Does not scale when the number of parameters to tune is increasing. Also will impose a regularity during the search which might be problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba9987-d774-4c53-bf92-305535662cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': (0.05, 0.1, 0.5, 1, 5),\n",
    "    'classifier__max_leaf_nodes': (3, 10, 30, 100)}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid)\n",
    "model_grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b520e3-553f-4568-8bbc-62d17657734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_grid_search.score(data_test, target_test)\n",
    "print(\n",
    "    f\"The test accuracy score of the grid-searched pipeline is: \"\n",
    "    f\"{accuracy:.2f}\"\n",
    ")\n",
    "\n",
    "print(f\"The best set of parameters is: \"\n",
    "      f\"{model_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f540c73-fd12-4613-9900-0d281b27217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To inspect all the test scores when using different parameters\n",
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0df74f-92de-46b9-8edc-1a8dc14842d2",
   "metadata": {},
   "source": [
    "Another method for Cross Validation with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b791bc4-8b5e-497a-bdb7-4ec9da96a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=10,\n",
    ").fit(data, target)\n",
    "grid_search.cv_results_\n",
    "\n",
    "# To sort the results into a dataframe\n",
    "results = (\n",
    "    pd.DataFrame(grid_search.cv_results_)\n",
    "    .sort_values(by=\"mean_test_score\", ascending=False)\n",
    ")\n",
    "\n",
    "results = results[\n",
    "    [c for c in results.columns if c.startswith(\"param_\")]\n",
    "    + [\"mean_test_score\", \"std_test_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1b884-0a3d-4ed9-9838-6fa4647ee8f1",
   "metadata": {},
   "source": [
    "## Automated using randomized-search\n",
    "A randomized-search allows a search with a fixed budget even with an increasing number of hyperparameters.\n",
    "\n",
    "**Note:** Random search (with RandomizedSearchCV) is typically beneficial compared to grid search (with GridSearchCV) to optimize 3 or more hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387b436f-020b-48ff-99cc-085ebe99a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "param_distributions = {\n",
    "    'classifier__learning_rate': loguniform(0.001, 10),\n",
    "    'columntransformer__standard-scaler__with_mean' : [True, False],\n",
    "    \"n_estimators\": randint(10, 30),\n",
    "    \"max_samples\": [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "model_random_search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_distributions, n_iter=500,\n",
    "    n_jobs=4, cv=5)\n",
    "model_random_search.fit(data_train, target_train)\n",
    "\n",
    "print(\"The best parameters are:\")\n",
    "print(model_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bf9e2-3693-4e50-a14a-4d2420e2ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# To see the results of all the params tried\n",
    "cv_results = pd.DataFrame(model_random_search.cv_results_)\n",
    "cv_results = cv_results.sort_values(by=\"rank_test_score\")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6aae4-15d7-4d30-b616-4e5e883ac54d",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f47457-e3e3-4e12-88d6-4fb6bd5102a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(data, target)\n",
    "linear_regression.coef_ #To get the coefficients\n",
    "linear_regression.intercept_ #To get the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f000ebb-7281-4ea9-87f7-7dc4480067ed",
   "metadata": {},
   "source": [
    "## Modelling Non-Linear Relationships (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfcb0c-d9f0-4f8f-91e3-018aa6ff69e5",
   "metadata": {},
   "source": [
    "We can use `sklearn.preprocessing.PolynomialFeatures` to generate polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f540fa-7e38-42ec-a564-cc1c4b69aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_regression = make_pipeline(\n",
    "    PolynomialFeatures(degree=3),\n",
    "    LinearRegression(),\n",
    ")\n",
    "polynomial_regression.fit(data, target)\n",
    "target_predicted = polynomial_regression.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f9387-f4ab-4c48-a44f-f63d01d093bf",
   "metadata": {},
   "source": [
    "### Using SVM (Kernels)\n",
    "See [documentation](https://scikit-learn.org/stable/modules/svm.html)\n",
    "\n",
    "Allows for  locally-based decision function instead of a global linear decision function.\n",
    "\n",
    "Kernel methods such as SVR are very efficient for small to medium datasets.\n",
    "\n",
    "For larger datasets with `n_samples >> 10_000`, it is often computationally\n",
    "more efficient to perform explicit feature expansion using\n",
    "`PolynomialFeatures` or other non-linear transformers from scikit-learn such\n",
    "as\n",
    "[KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)\n",
    "or\n",
    "[Nystroem](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b89b6-c8dd-4f0e-aea0-b3a1367c0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Using a Linear kernel\n",
    "svr = SVR(kernel=\"linear\")\n",
    "svr.fit(data, target)\n",
    "target_predicted = svr.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)\n",
    "\n",
    "# Using a non-linear (polynomial) kernel\n",
    "svr = SVR(kernel=\"poly\", degree=3)\n",
    "svr.fit(data, target)\n",
    "target_predicted = svr.predict(data)\n",
    "mse = mean_squared_error(target, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e63c13-7d3c-4cce-acf2-ec93dbe20f8c",
   "metadata": {},
   "source": [
    "## Regularization in Linear Models\n",
    "Use `RidgeCV` to create a linear model with ridge regression, and to identify the optimal `alpha` parameter value.\n",
    "\n",
    "Requires features to be **scaled**. If two features are as important, our model will boost the weights of features with small dispersion and reduce the weights of features with high dispersion. We recall that regularization forces weights to be closer. Therefore, we get an intuition that if we want to use regularization, dealing with rescaled data would make it easier to find an optimal regularization parameter and thus an adequate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3121c-e06c-4ae5-830e-c05a67f82930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2),\n",
    "    Ridge(alpha=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5adb6-0a0b-4a82-904c-5c6bca381385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-2, 0, num=20)\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      RidgeCV(alphas=alphas, store_cv_values=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7edc83-270b-4f81-a4b8-74d28ae8c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, random_state=1)\n",
    "cv_results = cross_validate(ridge, data, target,\n",
    "                            cv=cv, scoring=\"neg_mean_squared_error\",\n",
    "                            return_train_score=True,\n",
    "                            return_estimator=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde2410-0707-4332-9695-23784797865b",
   "metadata": {},
   "source": [
    "Plotting the effect of regularization term `alpha` on the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1084cd-523b-4f26-bba9-a082ccc237a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_alphas = [est[-1].cv_values_.mean(axis=0)\n",
    "              for est in cv_results[\"estimator\"]]\n",
    "cv_alphas = pd.DataFrame(mse_alphas, columns=alphas)\n",
    "\n",
    "cv_alphas.mean(axis=0).plot(marker=\"+\")\n",
    "plt.ylabel(\"Mean squared error\\n (lower is better)\")\n",
    "plt.xlabel(\"alpha\")\n",
    "_ = plt.title(\"Error obtained by cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8f952-cd30-4bd9-8c2c-9419dbbd160b",
   "metadata": {},
   "source": [
    "To check effects of the model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da748df-3303-4458-969d-40d9470f1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [estimator[-1].coef_[0] for estimator in cv_results[\"estimator\"]]\n",
    "coefs = pd.DataFrame(coefs, columns=feature_names)\n",
    "\n",
    "# Define the style of the box style\n",
    "boxplot_property = {\n",
    "    \"vert\": False,\n",
    "    \"whis\": 100,\n",
    "    \"patch_artist\": True,\n",
    "    \"widths\": 0.5,\n",
    "    \"boxprops\": dict(linewidth=3, color=\"black\", alpha=0.9),\n",
    "    \"medianprops\": dict(linewidth=2.5, color=\"black\", alpha=0.9),\n",
    "    \"whiskerprops\": dict(linewidth=3, color=\"black\", alpha=0.9),\n",
    "    \"capprops\": dict(linewidth=3, color=\"black\", alpha=0.9),\n",
    "}\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 35))\n",
    "_ = coefs.abs().plot.box(**boxplot_property, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add83ab-5c21-493f-b08f-d629d83656a2",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "`LogisticRegression` in `sklearn` is regularised by default, using the parameter `C` for regularisation, with smaller values indicating larger regularisation, and larger values indicating smaller regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1222f42-6827-4651-9474-e4391e846125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a2f457-dcf3-45dd-a404-018eeef9a458",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are non-parametric models and cannot hence extrapolate predictions beyond the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572694e6-2cd0-4a59-b2a5-c79b051fbd3d",
   "metadata": {},
   "source": [
    "## Classification Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81cdfb-70da-463b-9245-b2d249da816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(data_train, target_train)\n",
    "tree.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f253dd-90ca-441b-bc7b-67f345d55f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_decision_function(fitted_classifier, range_features, ax=None):\n",
    "    \"\"\"Plot the boundary of the decision function of a classifier.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    feature_names = list(range_features.keys())\n",
    "    # create a grid to evaluate all possible samples\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(*range_features[feature_names[0]], plot_step),\n",
    "        np.arange(*range_features[feature_names[1]], plot_step),\n",
    "    )\n",
    "\n",
    "    # compute the associated prediction\n",
    "    Z = fitted_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = LabelEncoder().fit_transform(Z)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # make the plot of the boundary and the data samples\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdBu\")\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f3187-4a10-40a5-9637-5f3583596537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the decision boundaries (2D)\n",
    "import seaborn as sns\n",
    "\n",
    "# create a palette to be used in the scatterplot\n",
    "palette = [\"tab:red\", \"tab:blue\", \"black\"]\n",
    "\n",
    "ax = sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1],\n",
    "                     hue=target_column, palette=palette)\n",
    "plot_decision_function(tree, range_features, ax=ax)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "_ = plt.title(\"Decision boundary using a decision tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fb8f8-ee1e-4b54-9002-691127e874a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16, 12))\n",
    "_ = plot_tree(tree, feature_names=data_columns,\n",
    "             class_names=tree.classes_, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99a9f3-ea32-4b98-9c53-3db3dc5c1eff",
   "metadata": {},
   "source": [
    "## Regression Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386f018-f0e3-47ce-accb-b206c951e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=1)\n",
    "tree.fit(data_train, target_train)\n",
    "target_predicted = tree.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a3025-a7c5-4737-9a62-dd0e3d3717d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "_ = plot_tree(tree, feature_names=data_columns, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728b604-73db-42a2-9fc9-db0acc0498c9",
   "metadata": {},
   "source": [
    "## Hyperparameters of Decision Trees\n",
    "The hyperparameters `min_samples_leaf`, `min_samples_split`,\n",
    "`max_leaf_nodes`, or `min_impurity_decrease` allows growing asymmetric trees\n",
    "and apply a constraint at the leaves or nodes level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5af8bf-d38a-413a-9401-c9ed079cba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Finding optimal max_depth parameter using GridSearchCV\n",
    "param_grid = {\"max_depth\": np.arange(2, 10, 1)}\n",
    "tree_clf = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid)\n",
    "tree_reg = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid)\n",
    "tree_clf.fit(data, target)\n",
    "tree_reg.fit(data, target)\n",
    "\n",
    "tree_clf.best_params_['max_depth'] #Optimal max_depth for Classification tree\n",
    "tree_reg.best_params_['max_depth'] #Optimal max_depth for Regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253ab75-80c8-458c-b47f-5c7af3cca44b",
   "metadata": {},
   "source": [
    "Plotting the test scores when using the different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f6806-2340-43be-9549-f8bc8d504981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"decisiontreeregressor__max_depth\": np.arange(1, 15)}\n",
    "search = GridSearchCV(tree, params, cv=10)\n",
    "cv_results_tree_optimal_depth = cross_validate(\n",
    "    search, data_numerical, target, cv=10, return_estimator=True, n_jobs=-1,\n",
    ")\n",
    "cv_results_tree_optimal_depth[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68393f3a-51f6-49d8-9833-7c3f300f8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "max_depth = [\n",
    "    estimator.best_params_[\"decisiontreeregressor__max_depth\"]\n",
    "    for estimator in cv_results_tree_optimal_depth[\"estimator\"]\n",
    "]\n",
    "max_depth = pd.Series(max_depth, name=\"max depth\")\n",
    "sns.swarmplot(max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea1f538-996a-45b0-bc6e-63ceb716e1b3",
   "metadata": {},
   "source": [
    "# Ensemble of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a22ccd-bde9-4f51-b646-28637a5a977b",
   "metadata": {},
   "source": [
    "## Bagging Models\n",
    "**Bagging** is a general strategy that can work with any base model (linear, trees, etc)\n",
    "- Bagging selects random subsets of the full data\n",
    "- Fit one model on each bagged subset, independent of the other fitted models\n",
    "- Average predictions\n",
    "\n",
    "**Random Forests** are bagged *randomized* decision trees\n",
    "- At each split: a random subset of features are selected\n",
    "- The best split is taken among the restricted subset\n",
    "- Extra randomisation **decorrelates** the prediction errors\n",
    "- Uncorrelated errors make bagging work better\n",
    "\n",
    "Each deep tree overfits individually but averaging the tree predictions reduces overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51b32a-2455-4093-be35-a0bd9d703259",
   "metadata": {},
   "source": [
    "A Bootstrap sample corresponds to a resampling with replacement, of the original dataset, to obtain a sample that is the same size as the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08556cd-a54c-4196-927b-6a60da66eab5",
   "metadata": {},
   "source": [
    "The parameter `n_estimators` controls how many models will be used, hence a larger value will give a more smooth bagging prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dfcec-a870-4b6d-bea7-f244f3a10d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=20, \n",
    "    random_state=0)\n",
    "\n",
    "cv_results = cross_validate(bagging_regressor, data, target, n_jobs=-1)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0222d5-df77-4806-8abe-80d09b1aa941",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "Main parameter to tune: `n_estimators`\n",
    "\n",
    "In general, the more trees in the forest, the better the statistical\n",
    "performance will be. However, it will slow down the fitting and prediction\n",
    "time. The goal is to balance computing time and statistical performance when\n",
    "setting the number of estimators when putting such learner in production.\n",
    "\n",
    "For random forests, it is possible to control the amount of randomness for\n",
    "each split by setting the value of `max_features` hyperparameter:\n",
    "\n",
    "- `max_feature=0.5` means that 50% of the features are considered at each\n",
    "  split;\n",
    "- `max_features=1.0` means that all features are considered at each split\n",
    "  which effectively disables feature subsampling.\n",
    "  \n",
    " By default, `RandomForestRegressor` disables feature subsampling while\n",
    "`RandomForestClassifier` uses `max_features=np.sqrt(n_features)`. These\n",
    "default values reflect good practices given in the scientific literature.\n",
    "\n",
    "However, `max_features` is one of the hyperparameters to consider when tuning\n",
    "a random forest:\n",
    "- too much randomness in the trees can lead to underfitted base models and\n",
    "  can be detrimental for the ensemble as a whole,\n",
    "- too few randomness in the trees leads to more correlation of the prediction\n",
    "  errors and as a result reduce the benefits of the averaging step in terms\n",
    "  of overfitting control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55b4d8-a755-4753-8a94-ed331bf93312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=50, n_jobs=2, random_state=0)\n",
    "\n",
    "scores = cross_val_score(random_forest, data, target, n_jobs=-1)\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99966df6-fd54-4a29-8285-ad488b49cc13",
   "metadata": {},
   "source": [
    "## Boosted Models\n",
    "### Boosting\n",
    "**Traditional Boosting:** Adaptive Boosting / `AdaBoost`\n",
    "- Mispredicted **samples are re-weighted** at each step\n",
    "- Can use any base model that accepts `sample_weight`\n",
    "\n",
    "### Gradient Boosting\n",
    "**Gradient Boosting:** `HistGradientBoosting`\n",
    "- Each base model predicts the **residual error** of previous models (according to some loss function)\n",
    "\n",
    "Differences between `HistGradientBoosting` and `GradientBoosting`\n",
    " - `GradientBoosting`\n",
    "    - Implements the traditional method, requires sortings of the weights\n",
    "    - Too slow for n_samples > 10,000\n",
    " - `HistGradientBoosting`\n",
    "    - Discretize numerical features (256 levels on default)\n",
    "    - Efficient multi-core implementation\n",
    "    - Much faster when n_samples is large\n",
    "\n",
    "(Gradient) Boosting fits trees sequentially, each shallow tree underfits individually and sequentially adding trees reduces underfitting\n",
    "\n",
    "Gradient boosting tends to perform slightly better than bagging and random forest, and furthermore shallow trees predict faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf3a3e9-fd44-49d5-a22c-6c8361315774",
   "metadata": {},
   "source": [
    "## Traditional Boosting (AdaBoost)\n",
    "**Cons:** Can overfit if number of estimators used is not optimal (too much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f7b0b-0d70-4c3a-a09a-767ffe8c10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=3, algorithm=\"SAMME\",\n",
    "                              random_state=0)\n",
    "adaboost.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409ff46-ec3f-4df7-9346-252080251036",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "Main parameters: `max_depth`, and `learning_rate`\n",
    "\n",
    "Gradient boosting will always improve when increasing the number of trees in the ensemble, but will reach a plateau where adding new trees will make fitting and scoring slower.\n",
    "\n",
    "To avoid adding new unnecessary tree, gradient boosting offers an **early-stopping option**. (Instead of tuning `n_estimators`)  \n",
    "Internally, the algorithm will use an out-of-sample set to compute the statistical performance of the model at each addition of a tree.  \n",
    "Thus, if the statistical performance are not improving for several iterations, it will stop adding trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593d80c-3a55-4007-a853-d9ed260c3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "grad_boost = GradientBoostingRegressor(n_estimators=1000, n_iter_no_change=5) \n",
    "grad_boost.fit(X_train, y_train)\n",
    "grad_boost.n_estimators_ #To check the number of trees used in the model\n",
    "\n",
    "cv_results_gbdt = cross_validate(\n",
    "    grad_boost, \n",
    "    data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c15ce2-3382-494f-8cb7-836b2b39d200",
   "metadata": {},
   "source": [
    "## Histogram Gradient Boosting (Faster)\n",
    "Implementation is similar to **LightGBM** whereby the number of splits considered within the tree building is reduced by **binning the data** before passing them into the gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b070b-9a55-4d98-a7f8-b8bc932d7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "histogram_gradient_boosting = HistGradientBoostingRegressor(max_iter=1000, random_state=0, early_stopping=True)\n",
    "cv_results_hgbdt = cross_validate(\n",
    "    gradient_boosting, data, target, scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b270fdf-16d7-4f24-97ca-8b05d779f10e",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "Have to evaluate model performance against a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd62f90-b1c6-421e-bfe2-89a980b81dc7",
   "metadata": {},
   "source": [
    "## Cross Validation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca9294-f84b-48ea-b5c1-ab6a0db799c7",
   "metadata": {},
   "source": [
    "### Stratification\n",
    "To create K-Folds with stratified data in each fold, perserving the proportion of classes within each fold  \n",
    "Ensures that no class is left out during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412aa84-356b-4bdc-97d1-bfe3f2287678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "results = cross_validate(model, data, target, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75d899-ad86-487f-b809-c4517b35ca8f",
   "metadata": {},
   "source": [
    "### Shuffling the Data\n",
    "Using `KFold` with `shuffle=True` will allow the data to be shuffled before splitting, ensuring that any ordered data will not be taken into account during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df5309-fcd5-48ea-96d7-1e4885ae34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(shuffle=True)\n",
    "results = cross_validate(model, data, target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d1a0a-b54a-4fae-9576-fcfa2f419f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e145146-1fc4-4c13-8f79-4d0df9e7e723",
   "metadata": {},
   "source": [
    "### Handling Non-iid Data (Time Series)\n",
    "Have to split in order, using the first n% of data to predict the remaining (100-n)% of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07050012-2fcc-4ac4-bd2c-addf558dbbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "groups = data.index.to_period(\"Q\")\n",
    "cv = TimeSeriesSplit(n_splits=groups.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733df49-3f3d-4de5-a71d-3ab7484306dd",
   "metadata": {},
   "source": [
    "### Grouped Data\n",
    "Use a grouped cross validation strategy to account for groups in the dataset, else the results will be overoptimistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aeae93-b722-44c0-9664-7be3d6304242",
   "metadata": {},
   "source": [
    "### Nested Cross-Validation\n",
    "To both evaluate the model and tune the model's hyperparameters\n",
    "\n",
    "When optimizing parts of the machine learning pipeline (e.g.\n",
    "hyperparameter, transform, etc.), one needs to use nested cross-validation to\n",
    "evaluate the statistical performance of the predictive model. Otherwise, the\n",
    "results obtained without nested cross-validation are over-optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a518ad-9868-4951-9902-05bcbdba22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "\n",
    "# Declare the inner and outer cross-validation\n",
    "inner_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Inner cross-validation for parameter search\n",
    "model = GridSearchCV(\n",
    "    estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, n_jobs=-1)\n",
    "\n",
    "# Outer cross-validation to compute the testing score\n",
    "test_score = cross_val_score(model, data, target, cv=outer_cv, n_jobs=-1)\n",
    "print(f\"The mean score using nested cross-validation is: \"\n",
    "      f\"{test_score.mean():.3f} +/- {test_score.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706f3b7-f87e-498f-b001-2ae4e4a53b1d",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87935cd4-11e3-4941-9efa-53134c23a004",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42444bb6-ee14-4bb7-bf46-c4deb8b6ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(target_test, target_predicted)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d68d340-3c17-4eea-b9d9-4f0feb9f5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "_ = plot_confusion_matrix(classifier, data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee96d1f-5183-4aa2-9cef-e2eb340d11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(target_test, target_predicted, pos_label=\"donated\")\n",
    "recall = recall_score(target_test, target_predicted, pos_label=\"donated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34eb8a-7d09-483c-b0c5-7694f9f857d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy = balanced_accuracy_score(target_test, target_predicted)\n",
    "print(f\"Balanced accuracy: {balanced_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971f49a-6ddc-46b0-ae6a-aa04a64e15c9",
   "metadata": {},
   "source": [
    "The function `cross_validate` allows the computation of multiple scores by passing a list of string or scorer to the parameter `scoring`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395ccaa-6abc-49bd-8805-81915d822c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scoring = [\"accuracy\", \"balanced_accuracy\"]\n",
    "\n",
    "scores = cross_validate(tree, data, target, cv=cv, scoring=scoring)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74e647-9f36-4e51-9a2d-91d0caed98a9",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184f7b0-8561-4109-b2ee-db1b76930c38",
   "metadata": {},
   "source": [
    "**Loss Functions:**\n",
    "- Mean Squared Error (MSE)\n",
    "- Mean Absolute Error (MAE)\n",
    "- $R^2$ Score: represents the proportion of variance explained by the model (default score in `sklearn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ec08a-703d-408a-b5f9-de7d07277208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
